{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run remote VSCode / code-server on SPCS\n",
    "- Configure Snowflake Roles & Resources for SPCS\n",
    "- Configure dockerfile for code-server & non-root miniconda access\n",
    "- Configure SPCS spec / service file\n",
    "- Push service file to @specs stage\n",
    "- Create service from staged spec file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Import packages\n",
    "Note: The following snowflake.core packages are only required if performing DDL operations in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/opt/conda/bin/python3\n",
    "import os\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.core._common import CreateMode\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core.stage import (\n",
    "    Stage,\n",
    "    StageEncryption,\n",
    "    StageDirectoryTable,\n",
    ")\n",
    "\n",
    "from snowflake.core.grant import (\n",
    "    Grant,\n",
    "    Grantees,\n",
    "    Privileges,\n",
    "    Securables,\n",
    ")\n",
    "\n",
    "from snowflake.core.role import Role\n",
    "from snowflake.core.database import Database\n",
    "from snowflake.connector import connect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optional: Configure connection using local environment vars\n",
    "* place .env file with env vars in base directory of workspace \n",
    "* dotenv loads environment vars into python kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pip install python-dotenv\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_dotenv\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_PARAMETERS_ACCOUNT_ADMIN = {\n",
    "    \"account\": os.environ[\"snowflake_account\"],\n",
    "    \"user\": os.environ[\"snowflake_user\"],\n",
    "    \"password\": os.environ[\"snowflake_password\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a SnowflakeConnection instance\n",
    "connection_acct_admin = connect(**CONNECTION_PARAMETERS_ACCOUNT_ADMIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Note that the following steps can be performed with SQL or Python\n",
    "- SQL generally more legible / fewer lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # create a root as the entry point for all object\n",
    "    root = Root(connection_acct_admin)\n",
    "\n",
    "    # CREATE ROLE CONTAINER_USER_ROLE\n",
    "    root.roles.create(Role(\n",
    "        name='CONTAINER_USER_ROLE',\n",
    "        comment='My role to use container',\n",
    "    ))\n",
    "\n",
    "    # GRANT CREATE DATABASE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE\n",
    "    # GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "    # GRANT CREATE COMPUTE POOL ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "    # GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "    # GRANT MONITOR USAGE ON ACCOUNT TO  ROLE  CONTAINER_USER_ROLE;\n",
    "    # GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE CONTAINER_USER_ROLE;\n",
    "    root.grants.grant(Grant(\n",
    "        grantee=Grantees.role('CONTAINER_USER_ROLE'),\n",
    "        securable=Securables.current_account,\n",
    "        privileges=[Privileges.create_database,\n",
    "                    Privileges.create_warehouse,\n",
    "                    Privileges.create_compute_pool,\n",
    "                    Privileges.create_integration,\n",
    "                    Privileges.monitor_usage,\n",
    "                    Privileges.bind_service_endpoint\n",
    "                    ],\n",
    "    ))\n",
    "\n",
    "    # GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE CONTAINER_USER_ROLE;\n",
    "    root.grants.grant(Grant(\n",
    "        grantee=Grantees.role('CONTAINER_USER_ROLE'),\n",
    "        securable=Securables.database('snowflake'),\n",
    "        privileges=[Privileges.imported_privileges\n",
    "                    ],\n",
    "    ))\n",
    "\n",
    "    # grant role CONTAINER_USER_ROLE to role ACCOUNTADMIN;\n",
    "    root.grants.grant(Grant(\n",
    "        grantee=Grantees.role('ACCOUNTADMIN'),\n",
    "        securable=Securables.role('CONTAINER_USER_ROLE')\n",
    "    ))\n",
    "\n",
    "    # USE ROLE CONTAINER_USER_ROLE\n",
    "    root.session.use_role(\"CONTAINER_USER_ROLE\")\n",
    "\n",
    "    # CREATE OR REPLACE DATABASE CONTAINER_HOL_DB;\n",
    "    root.databases.create(Database(\n",
    "        name=\"CONTAINER_HOL_DB\",\n",
    "        comment=\"This is a Container Quick Start Guide database\"\n",
    "    ), mode=CreateMode.or_replace)\n",
    "\n",
    "    # CREATE OR REPLACE WAREHOUSE CONTAINER_HOL_WH\n",
    "    #   WAREHOUSE_SIZE = XSMALL\n",
    "    #   AUTO_SUSPEND = 120\n",
    "    #   AUTO_RESUME = TRUE;\n",
    "    root.warehouses.create(Warehouse(\n",
    "        name=\"CONTAINER_HOL_WH\",\n",
    "        warehouse_size=\"XSMALL\",\n",
    "        auto_suspend=120,\n",
    "        auto_resume=\"true\",\n",
    "        comment=\"This is a Container Quick Start Guide warehouse\"\n",
    "    ), mode=CreateMode.or_replace)\n",
    "\n",
    "    # CREATE STAGE IF NOT EXISTS specs\n",
    "    # ENCRYPTION = (TYPE='SNOWFLAKE_SSE');\n",
    "    root.databases['CONTAINER_HOL_DB'].schemas[CONNECTION_PARAMETERS_ACCOUNT_ADMIN.get(\"schema\")].stages.create(\n",
    "        Stage(\n",
    "            name=\"specs\",\n",
    "            encryption=StageEncryption(type=\"SNOWFLAKE_SSE\")\n",
    "    ))\n",
    "\n",
    "    # CREATE STAGE IF NOT EXISTS volumes\n",
    "    # ENCRYPTION = (TYPE='SNOWFLAKE_SSE')\n",
    "    # DIRECTORY = (ENABLE = TRUE);\n",
    "    root.databases['CONTAINER_HOL_DB'].schemas[CONNECTION_PARAMETERS_ACCOUNT_ADMIN.get(\"schema\")].stages.create(\n",
    "        Stage(\n",
    "            name=\"volumes\",\n",
    "            encryption=StageEncryption(type=\"SNOWFLAKE_SSE\"),\n",
    "            directory_table=StageDirectoryTable(enable=True)\n",
    "    ))\n",
    "    # create collection objects as the entry\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    connection_acct_admin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Setup Steps (Skipped/pre-existing for the purposes of this demo)\n",
    "1. setting up security integrations\n",
    "  - configure ingress -- base case is an oauth config to snowservices_ingress\n",
    "2. setting up network rule\n",
    "  - defines ingress/egress IPs, ports\n",
    "  - typically execute only once, during initial SPCS configuration\n",
    "3. setting up external access integration\n",
    "  - param for external access integration is previously created network rule\n",
    "4. Create image registry within SPCS\n",
    "\n",
    "Ref: (https://quickstarts.snowflake.com/guide/intro_to_snowpark_container_services/index.html?index=..%2F..index#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test connection for container user role\n",
    "- container services are scoped to specific database, schema, and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_PARAMETERS_CONTAINER_USER_ROLE = {\n",
    "    \"account\": os.environ[\"snowflake_account\"],\n",
    "    \"user\": os.environ[\"snowflake_user\"],\n",
    "    \"password\": os.environ[\"snowflake_password\"],\n",
    "    \"role\": \"CONTAINER_USER_ROLE\",\n",
    "    \"warehouse\": \"CONTAINER_HOL_WH\",\n",
    "    \"Database\": \"CONTAINER_HOL_DB\",\n",
    "    \"Schema\": \"public\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect as CONTAINER_USE_ROLE\n",
    "connection_container_user_role = connect(**CONNECTION_PARAMETERS_CONTAINER_USER_ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Registry Usage\n",
    "- MFA based login command, below\n",
    "- Create image registry in prior step, or use existing; see reference (https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-registry-repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! snow spcs image-registry token --format=JSON | \\\n",
    "docker login sfsenorthamerica-demo-cgoyette.registry.snowflakecomputing.com -u 0sessiontoken --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build docker image - linuxserver.io/code-server\n",
    "- starting with base image from [lscr.io](https://docs.linuxserver.io/images/docker-code-server/#application-setup)\n",
    "- added steps for miniconda installation & expose conda on path for non-root users\n",
    "  - Reference: https://stackoverflow.com/questions/58269375/how-to-install-packages-with-miniconda-in-dockerfile\n",
    "- see included dockerfile in /src/code-server-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Docker Image -- Run these commands in a terminal\n",
    "! cd .../sfguide-intro-to-snowpark-container-services/src/code-server-2\n",
    "## the following command will build an image from the dockerfile in the current directory, and name it cgoyette/code-server-conda\n",
    "! docker build -t cgoyette/code-server-conda ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Observe Image in docker repo\n",
    "! docker image list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional / Best practice: Tag image in Docker\n",
    "- note image ID below, from prior step / local image after pull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker tag cgoyette/code-server-conda sfsenorthamerica-demo-cgoyette.registry.snowflakecomputing.com/container_hol_db/public/image_repo/code-server-conda:amd64-latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker: push created image to your SPCS registry\n",
    "After completing this step, use 'docker image list' to check that it exists in your registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## note: using tag from prior step\n",
    "! docker push sfsenorthamerica-demo-cgoyette.registry.snowflakecomputing.com/container_hol_db/public/image_repo/code-server:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Test running the image, locally\n",
    "- CG note: Works as expected\n",
    "- Service available on local host\n",
    "\n",
    "Note that this requires that the image architecture matches the architecture of your local machine. Docker buildx is also an option, if you have a Mac laptop with an ARM-based M chipset, and want to test locally without managing multiple images with different architectures for testing purposes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "docker run -d \\\n",
    "  --name=code-server \\\n",
    "  -e PUID=1000 \\\n",
    "  -e PGID=1000 \\\n",
    "  -e TZ=Etc/UTC \\\n",
    "  -e PASSWORD=password \\ #not secure\n",
    "  -e SUDO_PASSWORD=password \\ #not secure\n",
    "  -e PROXY_DOMAIN=code-server.my.domain \\\n",
    "  -e DEFAULT_WORKSPACE=/config/workspace  \\\n",
    "  -p 8443:8443 \\\n",
    "  -v ~/config:/config \\\n",
    "  --restart unless-stopped \\\n",
    "  lscr.io/linuxserver/code-server:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Push the Spec YAML\n",
    "Services in Snowpark Container Services are defined using YAML files. These YAML files configure all of the various parameters, etc. needed to run the containers within your Snowflake account. \n",
    "\n",
    "These YAMLs support a large number of configurable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref/Example: service spec for code server\n",
    "* see yaml file\n",
    "* /src/code-server-2/code-server-2.yaml  \n",
    "* [Service Spec Reference](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/specification-reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cgoyette/Documents/GitHub/sfguide-intro-to-snowpark-container-services'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push spec file to stage\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Snowcli to push the yaml spec to your Database's @specs stage \n",
    "\n",
    "Note: if you would like to modify an existing service, take the following steps:\n",
    "1. Suspend existing/running service\n",
    "2. Update spec yaml file as needed\n",
    "3. Perform the following copy operation to overwrite existing spec file\n",
    "4. Resume service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put \u001b[4;94mfile:///Users/cgoyette/Documents/GitHub/snowpark-container-services-code-server/src/code-server-gpu/code-server-gpu.yaml\u001b[0m @specs \u001b[33mauto_compress\u001b[0m=\u001b[35mfalse\u001b[0m \u001b[33mparallel\u001b[0m=\u001b[1;36m4\u001b[0m \u001b[33moverwrite\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
      "\u001b[?25l+------------------------------------------------------------------------------+\n",
      "|\u001b[1m        \u001b[0m|\u001b[1m        \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m          \u001b[0m|\u001b[1m \u001b[0m\u001b[1msource_\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget_c\u001b[0m\u001b[1m \u001b[0m|\u001b[1m        \u001b[0m|\u001b[1m         \u001b[0m|\n",
      "|\u001b[1m        \u001b[0m|\u001b[1m        \u001b[0m|\u001b[1m \u001b[0m\u001b[1msource_\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget_s\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mcompres\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mompressi\u001b[0m\u001b[1m \u001b[0m|\u001b[1m        \u001b[0m|\u001b[1m         \u001b[0m|\n",
      "|\u001b[1m \u001b[0m\u001b[1msource\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1msize   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mize     \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1msion   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mon      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mstatus\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mmessage\u001b[0m\u001b[1m \u001b[0m|\n",
      "|--------+--------+---------+----------+---------+----------+--------+---------|\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K+------------------------------------------------------------------------------+\n",
      "|\u001b[1m         \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m \u001b[0m\u001b[1msource_\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget_\u001b[0m\u001b[1m \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m        \u001b[0m|\n",
      "|\u001b[1m         \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m \u001b[0m\u001b[1msource_\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget_\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mcompres\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mcompres\u001b[0m\u001b[1m \u001b[0m|\u001b[1m         \u001b[0m|\u001b[1m \u001b[0m\u001b[1mmessag\u001b[0m\u001b[1m \u001b[0m|\n",
      "|\u001b[1m \u001b[0m\u001b[1msource \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1msize   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1msize   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1msion   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1msion   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mstatus \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1me     \u001b[0m\u001b[1m \u001b[0m|\n",
      "|---------+---------+---------+---------+---------+---------+---------+--------|\n",
      "| code-se | code-se | 694     | 694     | NONE    | NONE    | UPLOADE |        |\n",
      "| rver-gp | rver-gp |         |         |         |         | D       |        |\n",
      "| u.yaml  | u.yaml  |         |         |         |         |         |        |\n",
      "+------------------------------------------------------------------------------+\n",
      "\u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "#! snow object stage copy ./src/code-server-2/code-server-2.yaml @specs --overwrite --connection CONTAINER_hol\n",
    "! snow object stage copy ./src/code-server-gpu/code-server-gpu.yaml @specs --overwrite --connection CONTAINER_hol\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Create & test service"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "#SQL\n",
    "CREATE SERVICE CONTAINER_HOL_DB.PUBLIC.CODE_SERVER_GPU\n",
    "IN COMPUTE POOL CONTAINER_HOL_POOL\n",
    "from @specs\n",
    "specification_file='code-server-gpu.yaml'\n",
    "external_access_integrations = (ALLOW_ALL_EAI);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check status\n",
    "-- see scratch.sql for sql commands, and to execute from worksheet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---run in terminal\n",
    "CALL SYSTEM$GET_SERVICE_STATUS('CONTAINER_HOL_DB.PUBLIC.CODE_SERVER_3');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CALL SYSTEM$GET_SERVICE_LOGS('CONTAINER_HOL_DB.PUBLIC.JUPYTER_SNOWPARK_SERVICE', '0', 'jupyter-snowpark',10);\n",
    "\n",
    "SHOW ENDPOINTS IN SERVICE JUPYTER_SNOWPARK_SERVICE;\n",
    "\n",
    "    # --- After we make a change to our Jupyter notebook,\n",
    "    # --- we will suspend and resume the service\n",
    "    # --- and you can see that the changes we made in our Notebook are still there!\n",
    "    # ALTER SERVICE CONTAINER_HOL_DB.PUBLIC.JUPYTER_SNOWPARK_SERVICE SUSPEND;\n",
    "\n",
    "    # ALTER SERVICE CONTAINER_HOL_DB.PUBLIC.JUPYTER_SNOWPARK_SERVICE RESUME;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "### Access to running service\n",
    "The following command will return the exposed endpoint(s) of the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "#SQL \n",
    "SHOW ENDPOINTS IN SERVICE CODE_SERVER_2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the URL, and paste it in your browser.   \n",
    "You will be asked to login to your Snowflake, after which you should successfully see your code-server instance running, all inside of Snowflake!   \n",
    "\n",
    "Note, to access the service the user logging in must have the CONTAINER_USER_ROLE AND their default role cannot be ACCOUNTADMIN, SECURITYADMIN, or ORGADMIN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps / Testing: Upload and Modify a Notebook\n",
    "CG Note: the following is lifted from the quickstart, but the behavior described has been validated, e.g.,\n",
    "- One does not need to suspend or resume an SPCS service to observe new files copied to stage\n",
    "- **Note: in service spec yaml, set the user ID and group ID to the default UID/GID for the hosted service. For code-server, this is 911 for both.** This enables r/w access to/from the stage and files accessible via the service.\n",
    "\n",
    "---\n",
    "\n",
    "Notice that in our spec YAML file we mounted the @volumes/jupyter-snowpark internal stage location to our workspace/stage directory inside of our running container. \n",
    "\n",
    "What this means is that we will use our internal stage @volumes to persist and store artifacts from our container. If you go check out the @volumes stage in Snowsight, you'll see that when we created our jupyter_snowpark_service, a folder was created in our stage: @volumes/jupyter-snowpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, any file that is uploaded to @volumes/jupyter-snowpark will be available inside of our container in the /home/jupyter directory, and vice versa. Read more about volume mounts in the documentation. To test this out, let's upload the sample Jupyter notebook that is in our source code repo at .../sfguide-intro-to-snowpark-container-services/src/jupyter-snowpark/sample_notebook.ipynb. To do this you can either  \n",
    "1. Click on the jupyter-snowpark directory in Snowsight, click the blue + Files button and drag/browse to sample_notebook.ipynb. Click Upload. Navigate to your Jupyter service UI in your browser, click the refresh arrow and you should now see your notebook available!  OR   \n",
    "2. Upload sample_notebook.ipynb to @volumes/jupyter-snowpark using SnowCLI OR \n",
    "3. Upload sample_notebook.ipynb directly in your Jupyter service on the home screen by clicking the Upload button. If you now navigate back to @volumes/jupyter-snowpark in Snowsight, our run an ls @volumes/jupyter-snowpark SQL command, you should see your sample_notebook.ipynb file listed. Note you may need to hit the Refresh icon in Snowsight for the file to appear.  \n",
    "\n",
    "What we've done is now created a Jupyter notebook which we can modify in our service, and the changes will be persisted in the file because it is using a stage-backed volume. Let's take a look at the contents of our sample_notebook.ipynb. Open up the notebook in your Jupyter service:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_spcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
